

# 一、贝叶斯模型



## （一）简介



  NBM，是Naive Bayesian Model的缩写，即朴素贝叶斯模型。朴素贝叶斯分类器是一种流行多年的技术，尽管它也被称作“傻瓜贝叶斯”。朴素贝叶斯分类适用于特征空间的维数较高，而不能使用密度估计时。


## （二）前提假设


朴素贝叶斯模型假设给定一个类G=j,特征变量$X_{k}$是独立的：


$$
f_{j}(X)=\prod_{k=1}^{p} f_{j k}\left(X_{k}\right)\tag{1.1}
$$

虽然这个假定通常不正确，但是它确实能显著地简化估计：


* 各个类条件边际密度 $f_{jk}$ 可以分别使用一维核密度估计单独估计,这实际上是原始朴素贝叶斯过程的推广，它使用一元高斯来表示这些边缘。


* 如果X的分量$X_{j}$是离散的，则适当使用直方图估计，这提供了一种混合变量的无缝方式输入特征向量。



## （三）算法原理


朴素贝叶斯分类器使用一种与用于聚类的混合模型相同的概率生成模型，该模型假设语料库是由不同的类混合生成的。生成过程对每个观察到的文档应用一次，如下所示:


1.选择第一类(混合组分)$C_{r}$，先验概率$\alpha_{r} = P(C_{r})$。


2. 根据$C_{r}$的概率分布生成下一个文档。最常见的选择是伯努利分布和多项分布。


观察到的(训练和测试)数据被假定为生成过程的结果，并对生成过程的参数进行估计，以使生成过程创建的数据集的对数似然最大化。通常，仅使用训练数据来估计参数，因为训练数据包含关于生成每个文档的混合组件的身份的附加信息。随后，这些参数被用来估计从每个混合成分(类)生成每个未标记测试文档的概率。这导致了未标记文件的概率分类。


我们可以将朴素贝叶斯视为迭代期望最大化算法的简化，在该算法中，标签的存在允许在单个迭代中执行该方法。与聚类不同的是，分类中的训练过程使用了M-step(对有标记的数据)的快速应用，而测试实例的概率预测是E-step对未标记的测试实例的单一应用(估计后验概率)。此外，朴素贝叶斯分类器具有类似于聚类中使用的伯努利和多项模型。


## (四)主要模型


### 伯努利模型（The Bernoulli Model）


在伯努利模型中，假设只观察到文档中每个术语的存在或不存在。因此，忽略术语的频率，将文档的向量空间表示为稀疏二进制向量。伯努利模型假设词典中的第j项$t_{j}$以$p_{j}^{r}$的概率出现在由第r类(混合组件)生成的文档中。从混合组件生成文档的概率$P(\bar{Z}|C_{r})$,然后，由各项对应的d个不同伯努利概率的乘积给出的混合组分$C_{r}$生成文档Z的概率$P(Z|C_{r})$:



$$
P(\bar{Z}|C_{r})=\prod_{t_{j}\epsilon \bar{Z}}p_{j}^{(r)}\prod_{t_{j}\epsilon \bar{Z}}(1-p_{j}^{(r)}) \tag{1.2}
$$

这里有一个重要的假设，那就是对于类的选择，各种术语的存在或不存在是有条件地独立的。因此，可以将属性的联合概率表示为单个属性对应值的乘积。这个假设也被称为朴素贝叶斯假设，这也是该方法被称为朴素贝叶斯分类器的原因。使用“朴素”形容是因为这种近似在实际环境中通常不成立。


贝叶斯分类器训练阶段的主要任务是估计先验概率$\alpha_{r}$和类特有生成概率$p_{j}^{r}$的(最大似然)值。估计这些参数使观测数据具有由模型生成的最大似然值，然后用于执行未见测试实例的标签的预测。我们可以将这个过程总结如下:


* 训练阶段:仅使用训练数据估计参数$p_{j}^{r}$和$\alpha_{r}$的最大似然值。


* 预测阶段:使用参数的估价值来预测每个未标记测试实例的类。

首先执行训练阶段，然后执行预测阶段。然而，由于朴素贝叶斯分类器的预测阶段是理解它的关键，我们将在训练阶段之前介绍预测阶段。因此，下一节将假设在训练阶段已经学习了模型参数。

#### 预测阶段

预测阶段采用后验概率贝叶斯规则对实例进行预测。其基本思想是，学习者利用训练数据中每个类的累计频率来学习每个类的先验概率$\alpha _{r}=P(C_{r})$。随后，它需要估计在观察到标签未知的特定文档(用二进制表示$\bar{Z}=(z_{1}\cdots z_{d})$后验概率P$(C_{r}|\bar{Z})$。这个估计提供了测试实例$\bar{Z}$属于一个特定类的概率预测。

根据贝叶斯后验概率规则，第r类的混合分量$C_{r}$生成$\bar{Z}$的后验概率可估计为:

$$
P(C_{r}|Z)=\frac{P(C_{r})\cdot P(\bar{Z}|C_{r})}{P(\bar{Z})}\propto P(C_{r})\cdot P(\bar{Z}|C_{r}) \tag{1.3}
$$

在分母中使用比例常数而不是$P(\bar{Z})$，因为估计的概率只在多个类之间进行比较，以确定预测的类，而$P(\bar{Z})$与类无关。

这里一个重要的发现是条件方程右边的所有参数都可以用伯努利模型估计出来。我们利用伯努利分布进一步扩展上述方程中的关系如下:

$$
P(C_{r}|\bar{Z})\propto P(C_{r})\cdot P(\bar{Z}|C_{r})=\alpha _{r}\prod_{t_{j}\epsilon \bar{Z}}p_{j}^{(r)}\prod_{t_{j}\epsilon \bar{Z}}(1-p_{j}^{(r)})  \tag{1.4}
$$

需要注意的是右边的所有参数都是在下面讨论的训练阶段估计的。因此，现在每个类别的估计概率被预测到一个常数比例因子。具有最高后验概率的类被预测为相关类，尽管输出有时以概率的形式提供。值得注意的是，这个步骤与聚类中用于混合建模的E-step相同，只是它只适用于未标记的测试实例。

#### 训练阶段

贝叶斯分类器的训练阶段使用标记的训练数据估计参数的最大似然值。很明显，我们需要估计两组参数，这是每个混合成分的先验概率$\alpha_{r}$和伯努利生成参数$p_{j}^{(r)}$。可用于参数估计的统计数据包括属于r类$C_{r}$的已标记文档$n_{r}$的数量，以及包含$t_{j}$的属于$C_{r}$类的文档$m_{j}^{(r)}$的数量。这些参数的最大似然估计可表示为:

1.先验概率的估计:由于训练数据包含了语料库大小为n的第一个类的文档，因此，对该类的先验概率进行自然估计：

$$
\alpha_{r}=\frac{n_{r}}{n} \tag{1.5}
$$

如果语料库较小，在分子上加一个小值$\beta>0$，在分母上加一个小值$\beta * k$,有助于进行拉普拉斯平滑:

$$
\alpha_{r}=\frac{n_{r}+\beta }{n+k \cdot \beta } \tag{1.6}
$$

$\beta$的精确值包含了平滑的量，在实践中经常设置为1。当数据量很小时，这导致先验概率估计更接近于1/k，这是一个在缺乏足够数据的情况下的合理假设。

2.类条件混合参数的估计:类条件混合参数$p_{j}^{r}$的估计如下:

$$
p_{j}^{r}=\frac{m_{j}^{r}}{n_{r}} \tag{1.7}
$$

在类条件下的概率上使用拉普拉斯平滑尤为重要，因为特定的术语$t_{j}$甚至可能不会出现在第r类的训练文档中，尤其是当语料库很小的时候。在这种情况下，估计$p_{j}^{r}$的相应值为$\theta$。由于的乘法性质，在一个看不见的文档中出现术语总是会导致该类的估计概率为$\theta$。这样的预测往往是错误的，而且是由于对小的训练数据进行过拟合造成的。类条件概率估计的拉普拉斯平滑方法如下。设$d_{a}$为每个训练文档的二进制表示中1的平均个数，d为字典的大小。其基本思想是上式分子上加上拉普拉斯平滑参数$\gamma >0$，在分母上加上$d_{\gamma}/d_{a}$:

$$
p_{j}^{r}=\frac{m_{j}^{r}+\gamma }{n_{r}+d_{\gamma}/d_{a}} \tag{1.8}
$$

在实践中，$\gamma$的值通常设置为1。当训练数据量很小时，这个选择导致$p_{j}^{r}$的默认值为$d_{\gamma}/d$，这反映了文档集合的稀疏程度。

值得注意的是，贝叶斯分类器的训练阶段是混合模型中用于聚类的M-step的简化变体。这种简化是因为有标签的训练数据可以用来推断混合组件中文档的隶属关系。

### 多项式模型（Multinomial Model）

伯努利模型只使用文档中是否存在术语，多项式模型明确使用术语的频率。正如伯努利模型中的参数$p_{j}^{r}$表示某项在某一特定分量中是否被观察到的概率，多项模型中的参数$q_{jr}$表示项$t_{j}$在第r个混合分量中的分数存在，包括重复的影响。对于一个特定的混合组件，$q_{jr}$和的值为1，将所有项(即$\sum_{j=1}^{d}q_{jr}=1$)。

多项式混合模型的生成过程首先选择概率为$\alpha_{r}=P(C_{r})$的第一类(混合组分)。然后，它抛出已加载的骰子(属于第r类)L次，以生成带有L个令牌的文档(计算重复次数)。加载的骰子的面与项d的数量一样多，第j个面出现的概率由$q_{jr}$给出，对于属于该类的骰子。因此，如果掷骰子的次数是L次，那么每个面出现的次数就提供了每个术语在观察的文档中出现的次数。假设文档Z的频率向量为$(z_{1}\cdots z_{d})$，则第i个文档的生成概率为下列多项分布:

$$
P(\bar{Z}|C_{r})=\frac{(\sum_{j=1}^{d}z_{j})!}{z_{1}!z_{2}!\cdots z_{d}!}\prod_{j=1}^{d}(q_{ir})^{z_{j}}\propto \prod_{j=1}^{d}\prod_{j=1}^{d}(q_{ir})^{z_{j}} \tag{1.9}
$$

比例常数对固定的$\bar{Z}$和变化的类都成立，因为它只依赖于$\bar{Z}$，与$C_{r}$类无关。

多项式模型的预测和训练的整个过程与伯努利模型非常相似。与Bernoulli模型的情况一样，可以使用贝叶斯规则来推导出测试实例$\bar{Z}$属于$C_{r}$类的后验概率的估计值:

$$
P(C_{r}|\bar{Z})\propto P(C_{r})\cdot P(\bar{Z}|C_{r})\propto \alpha _{r}\prod_{j=1}^{d}(q_{jr})^{z_{j}} \tag{1.10}
$$

如果需要，可以通过确保所有类的后验概率和为1来推断比例常数。可将后验概率最大的类别预测为测试实例$\bar{Z}$的相关类别。

为了计算右侧的值，只需要在训练阶段估计参数$\alpha_{r}$和$q_{jr}$。训练数据中每个类的分数存在被用作$\alpha_{r}$的估计。如果需要的话，可以使用拉普拉斯平滑法。此外，如果$\nu(j,r)$是术语$t_{j}$在属于类r的文档中出现的次数(对单个文档中的重复次数给予相应的信用)，那么估计$q_{ir}$可以如下计算:

$$
q_{jr}=\frac{\nu(j,r) }{\sum_{j=1}^{d}\nu(j,r)} \tag{1.11}
$$

我们还可以将这个估计看作是与特定术语相对应的类中令牌数量的一部分。这与伯努利模型不同，伯努利模型将类条件下的概率估计为包含特定术语的类特定文档的部分。也可以使用拉普拉斯平滑来平滑估计。在这种情况下，我们在分子上加上一个小的$\gamma>0 $，在分母上加上$\gamma\cdot d$。由此得出以下估计:

$$
q_{jr}=\frac{\nu(j,r)+\gamma }{\sum_{j=1}^{d}\nu(j,r)+\gamma \cdot d} \tag{1.12}
$$

通常将γ设置为$\gamma$。这种类型的平滑偏差估计的概率，每个面在多项骰子掷向1/d，这意味着所有的项是平等的偏爱。在缺乏足够数据的情况下，这是一个合理的假设。

## （五）实际观察

有条件独立的天真假设在实际环境中从来都不是真的。尽管如此，实际的预测却出奇的可靠。使用更复杂的假设往往会导致数据过度拟合。

一个很自然的问题是，什么时候使用伯努利模型或多项模型更可取。请注意，伯努利模型同时使用了文档中存在和不存在的术语，但它没有使用术语频率。两个主要因素是:(1)每个文档的典型长度;(2)提取术语的词典的大小。对于相对于一个小词典具有非稀疏表示的简短文档，使用伯努利模型是有意义的。在简短的文档中，术语的重复次数是有限的，这减少了从包含频率信息中获得的增益。此外，如果词典大小非常小，并且向量空间表示是非稀疏的，那么即使文档中没有一个术语，也是信息丰富的。当文档表示是稀疏的时，关于缺少术语的信息是嘈杂的，这损害了伯努利模型。此外，忽略频率信息也会增加伯努利模型的不准确性。因此，在这种情况下使用多项式模型是有意义的。

## （六）使用朴素贝叶斯对输出进行排序

分类的预测问题并不总是以选择单个测试实例的类为前提。在许多情况下，一组测试实例$\bar{Z_{1}}\cdots \bar{Z_{n}}$,我们希望将它们按照属于某一类特别有价值的兴趣的倾向进行排序。这个问题与搜索引擎的排名问题密切相关。
考虑这样一种情况:汽车爱好者对标签为Cars的第r类感兴趣。如何使用训练有素的贝叶斯模型对测试文档$(\bar{Z_{1}}\cdots \bar{Z_{nt}})$进行排序?前面的讨论已经说明了如何估计每个测试实例$\bar{Z_{i}}$的$P(C_{r}|\bar{Z})$达到比例常数。在比较不同类别的概率时，这个比例因子并不相关，但在比较不同实例的预测时，这个比例因子是相关的，因为它在不同实例中是不同的。通过使用所有类的后验概率总和必须为1这一事实，每个测试实例的比例因子可以很容易地估计:

$$
\sum_{r=1}^{k}P(C_{r}|\bar{Z})=1 \tag{1.13}
$$

缩放后，在不同实例间比较第r类的后验概率归一化值，并按概率递减的顺序对文档进行排序。

## （七）朴素贝叶斯实例

下面，我们将提供一个贝叶斯模型的数值例子。我们将为伯努利模型和多项模型提供一个类似的例子，其中文档被分类为cars或cats。

### 伯努利模型

考虑以下包含四个训练文档和两个测试文档的语料库。语料库以二进制形式表示，其中忽略术语的频率:

$$
\left(\begin{array}{crrrrrr} 
& \text { lion } & \text { tiger } & \text { cheetah } & \text { jaguar } & \text { porsche } & \text { ferrari } & \text { Label } \\
\text { Train1 } & 1 & 1 & 1 & 1 & 0 & 0 & \text { Cats } \\
\text { Train2 } & 1 & 1 & 1 & 1 & 0 & 0 & \text { Cats } \\
\text { Train3 } & 0 & 0 & 0 & 1 & 1 & 1 & \text { Cars } \\
\text { Train4 } & 0 & 0 & 0 & 1 & 1 & 1 & \text { Cars } \\
\text { Test1 } & 1 & 1 & 1 & 1 & 1 & 1 & - \\
\text { Test2 } & 1 & 1 & 1 & 1 & 0 & 0 & -
\end{array}\right) \tag{1.14}
$$

为了便于说明，本词典只包含六个术语。每个实例的类标签显示在最后一列。前四个文档是培训文档，最后一列中为它们显示的标签是Cats和Cars。然而，最后两行对应于测试实例，因为它们的标签丢失了。
在下面，仅展示如何使用训练数据预测文档Test1的两个标签的概率。训练和预测阶段的步骤如下。训练:为了进行训练，需要估计先验概率和类条件概率。在$\beta=\gamma=1$时采用拉普拉斯平滑。先验概率估计为:

$$
P(Car)=\frac{2+\beta }{4+2\beta }=\frac{1}{2} 
$$

$$
P(Cat)=\frac{2+\beta }{4+2\beta }=\frac{1}{2} 
$$

接下来，我们需要估计伯努利分布的参数。我们首先展示如何估计P(lion|Cats)。四个训练文档中词汇的平均数量为14/4，词汇的总大小为d= 6。因此，拉普拉斯平滑所需的稀疏性因子为6×4/14=12/7。为了估计P(lion|Cats)，请注意，这一术语出现在两份关于猫的训练文件中。因此，这个伯努利参数的估计如下:

$$
P(Lion|Cats)=\frac{2+\gamma }{2+\frac{12\gamma }{7}}=\frac{21}{26} 
$$

通过使用相同的参数，我们可以显示如下:

$$
\begin{array}{l}
P(\text { lion } \mid \text { Cats })=\frac{21}{26}, P(\text { tiger } \mid \text { Cats })=\frac{21}{26}, P(\text { cheetah } \mid \text { Cats })=\frac{21}{26}, P(\text { jaguar } \mid \text { Cats })=\frac{21}{26} \\
P(\text { porsche } \mid \text { Cats })=\frac{7}{26}, P(\text { ferrari } \mid \text { Cats })=\frac{7}{26}
\end{array} 
$$

类似地，我们可以计算出cars的伯努利分布的参数如下:

$$
\begin{array}{l}
P(\text { lion } \mid \text { Cats })=\frac{21}{26}, P(\text { tiger } \mid \text { Cats })=\frac{21}{26}, P(\text { cheetah } \mid \text { Cats })=\frac{21}{26}, P(\text { jaguar } \mid \text { Cats })=\frac{21}{26} \\
P(\text { porsche } \mid \text { Cats })=\frac{7}{26}, P(\text { ferrari } \mid \text { Cats })=\frac{7}{26}
\end{array}
$$

注意，“jaguar”是唯一一个对这两个类别都有高概率的术语。这些估计的概率代表了贝叶斯分类器使用的整个训练模型。接下来，我们将展示如何使用这些估计的概率来预测test1。

预测:test1的预测阶段特别简单，因为它包含词汇表的所有术语。因此，类条件概率的计算方法如下:

$$
\begin{array}{r}
P(\text { Cats } \mid \text { Test1 }) \propto P(\text { Cats }) \cdot P(\text { lion } \mid \text { Cats }) \cdot P(\text { tiger } \mid \text { Cats }) \cdot P(\text { cheetah } \mid \text { Cats }) \\
\quad P(\text { jaguar } \mid \text { Cats }) \cdot P(\text { porsche } \mid \text { Cats }) \cdot P(\text { ferrari } \mid \text { Cats }) \\
=\frac{1}{2}\left(\frac{21}{26}\right)^{4}\left(\frac{7}{26}\right)^{2} \\
P(\text { Cars } \mid \text { Test1 }) \propto P(\text { Cars }) \cdot P(\text { lion } \mid \text { Cars }) \cdot P(\text { tiger } \mid \text { Cars }) \cdot P(\text { cheetah } \mid \text { Cars }) . \\
\quad P(\text { jaguar } \mid \text { Cars }) \cdot P(\text { porsche } \mid \text { Cars }) \cdot P(\text { ferrari } \mid \text { Cars }) \\
=\frac{1}{2}\left(\frac{21}{26}\right)^{3}\left(\frac{7}{26}\right)^{3}
\end{array}
$$


这些计算仅仅提供了一个比例常数的推论。我们还可以通过确保相应的概率和为1来计算每个类别的准确概率。利用这一关系，我们得到P(Cats|Test1) =3/4和P(Cars|Test1)=1/4。因此，测试实例更有可能属于catcategory。这是一个合乎逻辑的结论，因为文档中更多的术语属于cat类别。值得注意的是，为了得到合理的结果，拉普拉斯平滑是必不可少的。如果没有使用拉普拉斯平滑，那么两种结果的概率都是$\theta$，这将导致一个不确定的预测.

### 多项式模型

在多项式模型的情况下，假设文档项矩阵包含频率。因此，我们使用了与前一种情况非常相似的矩阵，只不过它也包含频率。对应矩阵如下图所示:

$$
\left(\begin{array}{crrrrrrr} 
& \text { lion } & \text { tiger } & \text { cheetah } & \text { jaguar } & \text { porsche } & \text { ferrari } & \text { Label } \\
\text { Train1 } & 2 & 2 & 1 & 2 & 0 & 0 & \text { Cats } \\
\text { Train2 } & 2 & 3 & 3 & 3 & 0 & 0 & \text { Cats } \\
\text { Train3 } & 0 & 0 & 0 & 1 & 1 & 1 & \text { Cars } \\
\text { Train4 } & 0 & 0 & 0 & 2 & 1 & 2 & \text { Cars } \\
\text { Test1 } & 2 & 2 & 2 & 3 & 1 & 1 & - \\
\text { Test2 } & 1 & 1 & 1 & 1 & 0 & 0 & -
\end{array}\right) \tag{1.15}
$$

先验概率的计算方法与前面完全相同。因此，每个类的先验概率可以估计为(1/2)。为了计算多项式参数，需要计算各个类中每个项出现的次数(包括同一文档中重复的效果)。下表总结了这一点:

$$
\left(\begin{array}{rrrrrrr} 
& \text { lion } & \text { tiger } & \text { cheetah } & \text { jaguar } & \text { porsche } & \text { ferrari } & \text { Total } \\
\text { Cats } & 4 & 5 & 4 & 5 & 0 & 0 & 18 \\
\text { Cars } & 0 & 0 & 0 & 3 & 2 & 3 & 8
\end{array}\right) \tag{1.16}
$$

最后一列包含该类在其所有文档上的令牌总数。现在我们需要计算每个多项式参数的概率。没有拉普拉斯平滑，我们可以通过简单地将每一行除以最后的总数，从上述计数中推导出这些参数。然而，用平滑法，我们需要每个分子加1，每个分母加6，因为字典里有6项。$q_{jr}$对应的值如下表所示:

$$
\left(\begin{array}{rrrrrrr} 
& \text { lion } & \text { tiger } & \text { cheetah } & \text { jaguar } & \text { porsche } & \text { ferrari } \\
\text { Cats } & \frac{5}{24} & \frac{6}{24} & \frac{5}{24} & \frac{6}{24} & \frac{1}{24} & \frac{1}{24} \\
\text { Cars } & \frac{1}{14} & \frac{1}{14} & \frac{1}{14} & \frac{4}{14} & \frac{3}{14} & \frac{4}{14}
\end{array}\right) \tag{1.17}
$$ 


请注意，每一行的和是1，因为它表示在选择一个特定位置的单词的多项式事件中，骰子的不同面的概率。可以使用这些估计的参数来进行预测。由于test1的频率向量为(2,2,2,3,1,1)，这些频率为每一项的概率参数的指数:

$$
\begin{array}{l}
P(\text { Cats } \mid \text { Test } 1) \propto \frac{1}{2}\left(\frac{5}{24}\right)^{2}\left(\frac{6}{24}\right)^{2}\left(\frac{5}{24}\right)^{2}\left(\frac{6}{24}\right)^{3}\left(\frac{1}{24}\right)\left(\frac{1}{24}\right) \\
P(\text { Cars } \mid \text { Test } 1) \propto \frac{1}{2}\left(\frac{1}{14}\right)^{2}\left(\frac{1}{14}\right)^{2}\left(\frac{1}{14}\right)^{2}\left(\frac{4}{14}\right)^{3}\left(\frac{3}{14}\right)\left(\frac{4}{14}\right)
\end{array}
$$


经化简和归一化，可以得到的Cats和Cars的概率分别约为0.94和0.06。因此，我们得到了相同的结论，除了在这种情况下预测更明确。这是因为测试文档中与猫相关的单词的频率更高。值得注意的是，为了得到合理的结果，拉普拉斯平滑是必不可少的。如果没有使用拉普拉斯平滑，那么两种结果的概率都是$\theta$，这将导致一个不确定的预测。多项式模型也不使用测试文档中没有的术语。尽管Test1包含所有术语，但文档test2不包含。如果使用多项式模型对Test2进行分类，则P(Cats|Test2)和P(Cars|Test2)都可以仅用“狮子”、“老虎”、“猎豹”和“美洲虎”的条件概率估计来表示。

## （八）半监督朴素贝叶斯

贝叶斯模型为有监督和无监督学习模型之间的联系提供了非常清晰的图像。在无监督学习中，混合组件表示一个聚类，而在有监督学习中，混合组件表示一个类。它们计算程序的差异是由无监督混合建模的缺陷造成的。标签在识别生成每个训练点的混合成分时很有用，这样可以方便地估计每个混合成分的参数。在没有标签的情况下，人们被迫使用一种迭代方法来概率预测与每个数据点相关的混合成分(E-step)和估计混合参数(M-step)。标签的存在简化了贝叶斯分类的学习过程，因为没有标签的数据不用于参数估计。此外，使用学习到的参数对未标记的实例进行E-step分类。


半监督学习在标记数据数量有限的情况下是有用的，因此在参数估计过程中加入未标记数据，以提高分类精度。在参数估计中使用未标记数据导致半监督方法是迭代的。半监督方法假设每个混合组件都与一个类相关联。每个类的标记点和未标记点由其混合组件生成。在初始化时，利用贝叶斯算法对标记实例估计每个混合成分的参数和先验概率。随后，迭代使用以下两组步骤:


1.(E-step): E-step使用贝叶斯后验概率规则估计未标记实例的概率。因此，e步的第一次迭代将产生与朴素贝叶斯算法计算的完全相同的概率。因此，E-step保持不变，但仅在迭代期间应用于未标记的数据，以预测它们的类成员关系。在EM算法中，我们使用e步骤中导出的软隶属度概率来将隶属度权重与未标记的实例关联起来。一个点在不同聚类中的隶属权之和为1，因为它们代表后验概率。半监督设置中对E-step的一个重要修改是，已标记实例的成员权值$\lambda>0$与它所属的类/集群关联，0与所有其他类关联。$\lambda$的值是(0，∞)中用户驱动的参数，用来调节监控级别。

2.(M-step): M-step与混合建模算法中所讨论的相同，除了它是在修改的成员权的帮助下执行的，其中标记的实例被赋予用户定义的权重$\lambda$。



这两个步骤被迭代到收敛。最后迭代中e步的概率预测可以用来预测类标签，这个变化的影响程度取决于$\lambda$的值。
λ参数控制标记数据和未标记数据的重要性之间的权衡。设$\lambda =\infty $会得到本节的朴素贝叶斯算法。$\lambda$的其他所有正值都提供了不同程度的监督，在这些监督中仍然需要迭代方法。在半监督分类应用中，通常选择$\lambda >1$是明智的，因为应该对每个有标记的点进行更大程度的加权。


请注意，在标记数据量非常小的情况下，λ的这种中间值通常可以胜过朴素贝叶斯方法。在有限的标记数据中，完全监督的朴素贝叶斯方法无法很好地估计标记数据中缺失项的条件概率。在半监督的情况下，这种概率的估计要稳健得多，因为相关混合成分中的未标记文档可以用于稳健估计。理解这一点的另一种方法是，未标记数据可以了解底层数据分布的形状，并确保标记数据只需要将该数据分布的学习集群映射到不同的标签。因此，大多数学习数据分布形状的“繁重工作”都是用未标记的数据完成的，只需要少量的数据就可以将该数据分布的密集分段(混合组件)映射到不同的类中。这里的自然假设是，类标签不会在数据的连续、密集和集群区域内突然更改。这种情况经常发生在真实的数据集中，因为真实的类分布具有自然的平滑性和聚集性。也可以构造混合成分数大于标记类数的半监督模型来学习特定区域内局部相邻的类分布。

半监督的另一个好处是，学习过程是特定于我们感兴趣的测试实例的。纯监督方法构建的模型比我们真正需要的模型更通用。这提供了基于Vapnik原则的半监督优势“在尝试解决某个问题时，不应该把更困难的问题作为中间步骤”。

我们可以通过解决较窄的问题并将学习过程调优到手边的特定测试实例来获得更好的结果。例如，如果一个小的训练数据集只包含每个类的几个实例，那么实例的数量就太小，无法稳健地估计先验概率。另一方面，如果一个大型测试数据中包含这些类的比例为9:1，那么半监督参数估计过程将使用这些额外的信息来分配更稳健的先验概率。如果一个不同的测试数据集以1:9的相反比例包含这些类，它将分配不同的先验概率。

该方法既可用于半监督聚类，又可用于半监督分类。半监督集群与半监督分类的应用略有不同，因为前者的监督更温和，其目标是创建一个具有外部输入的语义有意义的分区，而不是对实例进行标签。对于半监督的聚类应用程序，可以使用较小的$\lambda$值，使未标记数据的聚类结构更加重要。


### 参考文献
[1]Dr. Charu C. Aggarwal（2018）. Aggarwal Machine Learning  for Text:233-243

[2]Trevor Hastie，Robert Tibshirani，Jerome Friedman.(2008)The Elements ofStatistical Learning Data Mining, Inference, and Prediction Second Edition.210-212:


